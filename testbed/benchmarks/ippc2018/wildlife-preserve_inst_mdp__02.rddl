/////////////////////////////////////////////////////////////////////////////////
//                                                                             //
//                                                                             //
// RDDL MDP version of Wildlife Preserve instance #02 for IPC 2018 by Fei Fang //
// (feifang [at] cmu.edu), Thanh Hong Nguyen (thanhhng [at] umich.edu) and     //
// Thomas Keller (tho.keller [at] unibas.ch), based on the papers "When        //
// Security Games Go Green: Designing Defender Strategies to Prevent Poaching  //
// and Illegal Fishing" by Fei Fang, Peter Stone and Milind Tambe (IJCAI 2015) //
// and "Analyzing the Effectiveness of Adversary Modeling in Security Games"   //
// by Thanh H. Nguyen, Rong Yang, Amos Azaria, Sarit Kraus and Milind Tambe    //
// (AAAI 2013).                                                                //
//                                                                             //
//                                                                             //
/////////////////////////////////////////////////////////////////////////////////

instance wildlife-preserve_inst_mdp__02 {
    domain = wildlife-preserve_02_mdp;

    objects {
        ranger  : { r1 };
        poacher : { p1, p2 };
    };

    non-fluents {
        DEFENDER-REWARD(@a1) = 10.02;
        DEFENDER-PENALTY(@a1) = -4.05;
        DEFENDER-REWARD(@a2) = 10.98;
        DEFENDER-PENALTY(@a2) = -11.30;
        DEFENDER-REWARD(@a3) = 6.80;
        DEFENDER-PENALTY(@a3) = -3.87;
        DEFENDER-REWARD(@a4) = 10.64;
        DEFENDER-PENALTY(@a4) = -2.93;

        // correlation between attacker reward and defender penalty as well as
        // attacker penalty and defender reward is 1.00 for all poachers and all areas

        // weights for poacher p1 are: w1 = -10.14, w2 = 0.75, w3 = 0.43
        // reward for poacher p1 in area @a1 is: 4.05
        // penalty for poacher p1 in area @a1 is: -10.02
        // reward for poacher p1 in area @a2 is: 11.30
        // penalty for poacher p1 in area @a2 is: -10.98
        // reward for poacher p1 in area @a3 is: 3.87
        // penalty for poacher p1 in area @a3 is: -6.80
        // reward for poacher p1 in area @a4 is: 2.93
        // penalty for poacher p1 in area @a4 is: -10.64

        // weights for poacher p2 are: w1 = -13.05, w2 = 0.51, w3 = 0.39
        // reward for poacher p2 in area @a1 is: 4.05
        // penalty for poacher p2 in area @a1 is: -10.02
        // reward for poacher p2 in area @a2 is: 11.30
        // penalty for poacher p2 in area @a2 is: -10.98
        // reward for poacher p2 in area @a3 is: 3.87
        // penalty for poacher p2 in area @a3 is: -6.80
        // reward for poacher p2 in area @a4 is: 2.93
        // penalty for poacher p2 in area @a4 is: -10.64

        ATTACK-WEIGHT_0(p1, @a1) = 0.28029;
        ATTACK-WEIGHT_1(p1, @a1) = 0.00176;
        ATTACK-WEIGHT_2(p1, @a1) = 0.00001;
        ATTACK-WEIGHT_0(p1, @a2) = 42.52244;
        ATTACK-WEIGHT_1(p1, @a2) = 0.26690;
        ATTACK-WEIGHT_2(p1, @a2) = 0.00168;
        ATTACK-WEIGHT_0(p1, @a3) = 0.97774;
        ATTACK-WEIGHT_1(p1, @a3) = 0.00614;
        ATTACK-WEIGHT_2(p1, @a3) = 0.00004;
        ATTACK-WEIGHT_0(p1, @a4) = 0.09273;
        ATTACK-WEIGHT_1(p1, @a4) = 0.00058;
        ATTACK-WEIGHT_2(p1, @a4) = 0.00000;
        ATTACK-WEIGHT_0(p2, @a1) = 0.16521;
        ATTACK-WEIGHT_1(p2, @a1) = 0.00024;
        ATTACK-WEIGHT_2(p2, @a1) = 0.00000;
        ATTACK-WEIGHT_0(p2, @a2) = 4.70831;
        ATTACK-WEIGHT_1(p2, @a2) = 0.00689;
        ATTACK-WEIGHT_2(p2, @a2) = 0.00001;
        ATTACK-WEIGHT_0(p2, @a3) = 0.52401;
        ATTACK-WEIGHT_1(p2, @a3) = 0.00077;
        ATTACK-WEIGHT_2(p2, @a3) = 0.00000;
        ATTACK-WEIGHT_0(p2, @a4) = 0.07313;
        ATTACK-WEIGHT_1(p2, @a4) = 0.00011;
        ATTACK-WEIGHT_2(p2, @a4) = 0.00000;

        POACHER-REMEMBERS(p1, @1);
        POACHER-REMEMBERS(p1, @2);
        POACHER-REMEMBERS(p2, @1);
        POACHER-REMEMBERS(p2, @2);

    };

    init-state {
        ~was-defended(@a1,@1);
    };

    horizon = 60;

    discount = 1.0;
}